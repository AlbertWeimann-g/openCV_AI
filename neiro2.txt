istall neiro 

       pip install tensorflow opencv-python numpy matplotlib

data set
   import cv2
   import os

# Class objects
classes = ['red_circle', 'blue_square', 'green_triangle', 'person']
for cls in classes:
    os.makedirs(f'dataset/{cls}', exist_ok=True)

#cap

def capture_images(class_name, num_samples=100):
    cap = cv2.VideoCapture(0)
    count = 0
    
    while count < num_samples:
        ret, frame = cap.read()
        if not ret:
            continue
            
        cv2.imshow('Capture', frame)
        key = cv2.waitKey(1)
        
        if key == ord('s'):  # Нажмите 's' для сохранения
            cv2.imwrite(f'dataset/{class_name}/{count}.jpg', frame)
            count += 1
            print(f"Saved {class_name} sample {count}/{num_samples}")
            
        elif key == 27:  # ESC для выхода
            break
            
    cap.release()
    cv2.destroyAllWindows()
#cap objects
for cls in classes:
    print(f"Capturing {cls}...")
    capture_images(cls)

pip install labelImg
labelImg  # interface

data - training
import xml.etree.ElementTree as ET
import numpy as np

def parse_annotation(ann_dir, img_dir, labels=[]):
    all_imgs = []
    seen_labels = {}
    
    for ann in os.listdir(ann_dir):
        img = {'object': []}
        tree = ET.parse(os.path.join(ann_dir, ann))
        
        for elem in tree.iter():
            if 'filename' in elem.tag:
                img['filename'] = os.path.join(img_dir, elem.text)
            if 'width' in elem.tag:
                img['width'] = int(elem.text)
            if 'height' in elem.tag:
                img['height'] = int(elem.text)
            if 'object' in elem.tag:
                obj = {}
                for attr in list(elem):
                    if 'name' in attr.tag:
                        obj['name'] = attr.text
                        if obj['name'] in seen_labels:
                            seen_labels[obj['name']] += 1
                        else:
                            seen_labels[obj['name']] = 1
                        
                        if len(labels) > 0 and obj['name'] not in labels:
                            break
                        else:
                            img['object'] += [obj]
        all_imgs += [img]
    return all_imgs, seen_labels

# example
train_imgs, train_labels = parse_annotation('dataset/annotations', 'dataset/images', classes)
print(f"Found {len(train_imgs)} images with {train_labels} objects")


#model training

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

def create_model(num_classes):
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    
    for layer in base_model.layers:
        layer.trainable = False
        
    # adding layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=base_model.input, outputs=predictions)
    return model

model = create_model(len(classes))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#data repartion

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

train_generator = datagen.flow_from_directory(
    'dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    'dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

#save model
model.save('object_classifier.h5')

#installing model
from tensorflow.keras.models import load_model

model = load_model('object_classifier.h5')

#object in real time
def classify_object(frame):
    
    img = cv2.resize(frame, (224, 224))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)
    
    # classification
    predictions = model.predict(img)
    class_id = np.argmax(predictions)
    confidence = np.max(predictions)
    
    return classes[class_id], confidence

#robot 
import time
from motion.core import RobotControl

robot = RobotControl()
robot.connect()
robot.engage()

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        continue
    
    
    class_name, confidence = classify_object(frame)
    
    if confidence > 0.7:  # Порог уверенности
        print(f"Detected: {class_name} with confidence {confidence:.2f}")
        
        # coordinates
        if class_name == 'red_circle':
            dest = [100, 100, 100]
        elif class_name == 'blue_square':
            dest = [200, 100, 100]
        elif class_name == 'green_triangle':
            dest = [300, 100, 100]
        elif class_name == 'person':
            print("Person detected - pausing operation")
            robot.pause()
            time.sleep(5)  # Ждем 5 секунд
            robot.play()
            continue
            
        # move robot
        robot.moveL(dest)
        robot.controlGripper(True)
        time.sleep(1)
        robot.moveL([dest[0]+50, dest[1]+50, dest[2]])
        robot.controlGripper(False)
    
    cv2.imshow('Detection', frame)
    if cv2.waitKey(1) == 27:  # ESC для выхода
        break

cap.release()
cv2.destroyAllWindows()
robot.disengage()


#MODELSPEED
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('model_quant.tflite', 'wb') as f:
    f.write(tflite_model)

#Modelspeed use
import tflite_runtime.interpreter as tflite

interpreter = tflite.Interpreter(model_path='model_quant.tflite')
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

def classify_object_optimized(frame):
    img = cv2.resize(frame, (224, 224))
    img = img / 255.0
    img = np.expand_dims(img, axis=0).astype(np.float32)
    
    interpreter.set_tensor(input_details[0]['index'], img)
    interpreter.invoke()
    
    predictions = interpreter.get_tensor(output_details[0]['index'])
    class_id = np.argmax(predictions)
    confidence = np.max(predictions)
    
    return classes[class_id], confidence

#optionaly
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.5, 1.5],
    fill_mode='nearest',
    validation_split=0.2
)

------

for layer in base_model.layers[-20:]:
    layer.trainable = True
    
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#log 
import logging
from datetime import datetime

logging.basicConfig(
    filename=f'robot_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def process_object(class_name, confidence):
    logging.info(f"Processing {class_name} (confidence: {confidence:.2f})")
    
    try:
        if class_name == 'person':
            logging.warning("Person detected - safety protocol activated")
            robot.pause()
            time.sleep(5)
            robot.play()
            return
            
        # Остальная логика обработки
        # ...
        
        logging.info("Object processed successfully")
    except Exception as e:
        logging.error(f"Error processing object: {str(e)}")

#docker
FROM python:3.8-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Установка зависимостей для OpenCV
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    && rm -rf /var/lib/apt/lists/*

CMD ["python", "main.py"]

